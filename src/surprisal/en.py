# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VVJ9QKvzKqWP7p_aI1brTR7sDR00f5N
"""

"""
English surprisal computation and reading difficulty prediction.
"""

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from src.config import EN_GPT_MODEL


class EnglishSurprisalModel:
    """
    Wrapper for computing English word-level surprisal.
    """

    def __init__(self, model_name: str = EN_GPT_MODEL, device: str | None = None):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        self.device = device
        self.model.to(self.device)
        self.model.eval()

    @torch.no_grad()
    def sentence_surprisal(self, words: list[str]) -> list[float]:
        """
        Compute surprisal for a sequence of English words.
        The first word receives NaN.
        """
        surprisals = []
        context_ids = None

        for w in words:
            word_ids = self.tokenizer.encode(
                " " + w,
                add_special_tokens=False
            )

            if context_ids is None:
                context_ids = torch.tensor([word_ids], device=self.device)
                surprisals.append(np.nan)
                continue

            total_surprisal = 0.0
            ids = context_ids.clone()

            for token_id in word_ids:
                logits = self.model(ids).logits
                log_probs = torch.log_softmax(logits[0, -1], dim=-1)
                total_surprisal += -log_probs[token_id].item()

                ids = torch.cat(
                    [ids, torch.tensor([[token_id]], device=self.device)],
                    dim=1
                )

            surprisals.append(total_surprisal)
            context_ids = ids

        return surprisals


def sentence_to_words(sentence: str) -> list[str]:
    """
    Tokenize sentence by whitespace.
    """
    return [w for w in sentence.split() if w.strip()]


def predict_difficulty(
    sentence: str,
    model: EnglishSurprisalModel,
    b0: float,
    b_s: float,
) -> tuple[float | None, float | None]:
    """
    Predict reading difficulty for an English sentence using surprisal-only transfer.
    Difficulty per word:
        diff = b0 + b_s * surprisal

    Returns
    -------
    mean_difficulty, variance_difficulty
    """
    words = sentence_to_words(sentence)
    surprisals = model.sentence_surprisal(words)

    diffs = []
    for s in surprisals:
        if s is None or np.isnan(s):
            continue
        diffs.append(b0 + b_s * s)

    if len(diffs) == 0:
        return None, None

    return float(np.mean(diffs)), float(np.var(diffs))