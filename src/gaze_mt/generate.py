# -*- coding: utf-8 -*-
"""generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vI3f_7AtIiIHQm3zMvXCeG7WA3Qve3go
"""

"""
Candidate generation for gaze-informed machine translation.

- generate(): returns only candidate strings (backward compatible)
- generate_with_scores(): returns candidates + sequence-level scores (HF sequences_scores)
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from src.config import MT_RU_EN_MODEL, NUM_BEAMS


class MTGenerator:
    """
    RU -> EN translation candidate generator.
    """

    def __init__(
        self,
        model_name: str = MT_RU_EN_MODEL,
        device: str | None = None,
    ):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        self.device = device
        self.model.to(self.device)
        self.model.eval()

    @torch.no_grad()
    def generate(
        self,
        sentence: str,
        num_beams: int = NUM_BEAMS,
    ) -> list[str]:
        """
        Backward compatible: return only translations.
        """
        candidates, _ = self.generate_with_scores(sentence, num_beams=num_beams)
        return candidates

    @torch.no_grad()
    def generate_with_scores(
        self,
        sentence: str,
        num_beams: int = NUM_BEAMS,
    ) -> tuple[list[str], list[float]]:
        """
        Generate translation candidates and their sequence-level scores.

        Returns
        -------
        candidates : list[str]
        log_probs  : list[float]
            HuggingFace `sequences_scores` (higher is better).
        """
        inputs = self.tokenizer(
            sentence,
            return_tensors="pt",
            truncation=True,
        ).to(self.device)

        out = self.model.generate(
            **inputs,
            num_beams=num_beams,
            num_return_sequences=num_beams,
            early_stopping=True,
            output_scores=True,
            return_dict_in_generate=True,
        )

        candidates = [
            self.tokenizer.decode(seq, skip_special_tokens=True)
            for seq in out.sequences
        ]

        log_probs = out.sequences_scores.detach().cpu().tolist()

        return candidates, log_probs